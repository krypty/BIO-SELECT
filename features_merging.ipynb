{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Features merging using several lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here + highlight this notebook on the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "#np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: load the features lists from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.CSVFeaturesImporter import CSVFeaturesImporter\n",
    "\n",
    "group_name = \"mean_score\"\n",
    "\n",
    "importer = CSVFeaturesImporter(group_name)\n",
    "subsets = importer.load()\n",
    "print(subsets[\"features\"].keys())\n",
    "print(subsets[\"features_by_score\"][\"ReliefF\"][:5])\n",
    "\n",
    "\n",
    "features = subsets[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "def compute_similary_between_subsets(subsets, compare_func):\n",
    "    N_subsets = len(subsets)\n",
    "    similarity_matrix = np.zeros(shape=(N_subsets, N_subsets))\n",
    "\n",
    "    for i, j in itertools.product(range(N_subsets), range(N_subsets)):\n",
    "        if isinstance(subsets[0][0], int):\n",
    "            subset_i = set(subsets[i])\n",
    "            subset_j = set(subsets[j])\n",
    "        else:\n",
    "            subset_i = {i[0] for i in subsets[i]}\n",
    "            subset_j = {j[0] for j in subsets[j]}\n",
    "\n",
    "        similarity_matrix[i, j] = compare_func(subset_i, subset_j)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "def plot_feature_subsets_matrix(cm, alg_names, title, cmap=plt.cm.Blues):\n",
    "    title += \"\\n\" # add a little margin for the title\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.colorbar()\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(alg_names))\n",
    "    plt.xticks(tick_marks, alg_names, rotation=45)\n",
    "    plt.yticks(tick_marks, alg_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 + 0.1\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        text = \"%.2f\" % cm[i, j]\n",
    "        plt.text(j, i, text,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 backgroundcolor=\"white\",\n",
    "                 #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                 color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# plot the similarity matrices\n",
    "it = features.items()\n",
    "alg_names, features_subsets = zip(*features.items())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(features_subsets, compare_func=jaccard)\n",
    "plt.subplot(1,2,1)\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Jaccard similarity between two feature subsets\")\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(features_subsets, compare_func=intersection_count)\n",
    "plt.subplot(1,2,2)\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Intersection between two feature subsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_features_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.simple.SimpleUnionSubsetMerger import SimpleUnionSubsetMerger\n",
    "\n",
    "susm = SimpleUnionSubsetMerger(features_subsets)\n",
    "merged_features = susm.merge()\n",
    "n_all_features = sum([len(s) for s in features_subsets])\n",
    "\n",
    "print(\"Unique features (union of all subsets): %d over a total of %d \" % (len(merged_features), n_all_features))\n",
    "\n",
    "merged_features_lists.append((\"Union of all features\", merged_features, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subsets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def group_by_features(features):\n",
    "    from itertools import groupby\n",
    "    \n",
    "    def keyfunc(x): return x[0]\n",
    "    \n",
    "    list_of_lists_sorted = sorted(features, key=keyfunc)\n",
    "    grouped_list = [list(j) for i, j in groupby(list_of_lists_sorted, key=keyfunc)]\n",
    "    return grouped_list\n",
    "\n",
    "def mean_score_for_feature(a, n_algorithms):\n",
    "    feat_name, feat_scores = zip(*a)\n",
    "    feat_name = feat_name[0] # since name is the same for all tuples\n",
    "    \n",
    "    m = sum(feat_scores)/float(n_algorithms)\n",
    "    return (feat_name, m)\n",
    "    \n",
    "def keep_top_n(features, n):\n",
    "    n_algorithms = len(features)\n",
    "    \n",
    "    print(features[\"SVM\"][:5])\n",
    "    all_feats = []\n",
    "    for f in features.values():\n",
    "        all_feats.extend(f)\n",
    "    \n",
    "    \n",
    "    print(all_feats[:8])\n",
    "    grouped_list = group_by_features(all_feats)\n",
    "    grouped_list = [mean_score_for_feature(f, n_algorithms) for f in grouped_list]\n",
    "    \n",
    "    grouped_list = sorted(grouped_list, key=lambda x: x[1], reverse=True)\n",
    "    print(grouped_list[:8])\n",
    "    \n",
    "    return [x[0] for x in grouped_list[:n]]\n",
    "    \n",
    "    \n",
    "merged_features = keep_top_n(subsets[\"features\"], n=100)\n",
    "merged_features_lists.append((\"Keep Top N features\", merged_features, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def union_of_intersection_two_by_two(features):\n",
    "    sort_by_len_features = sorted(features.values(), key=lambda x:len(x), reverse=True)\n",
    "    print([len(f) for f in sort_by_len_features])\n",
    "    \n",
    "    def union(x, y):\n",
    "        inter = list(set(x).intersection(set(y)))\n",
    "        print(len(inter))\n",
    "        return inter\n",
    "    \n",
    "    lists_of_features = [([a[0] for a in f]) for f in sort_by_len_features]\n",
    "    \n",
    "    # keep the lists that contains at least 500 features\n",
    "    lists_of_features = filter(lambda x:len(x) > 500, lists_of_features)\n",
    "    \n",
    "    return reduce(union, lists_of_features)\n",
    "\n",
    "    \n",
    "merged_features = union_of_intersection_two_by_two(subsets[\"features_by_score\"])\n",
    "merged_features_lists.append((\"Union of intersections\", merged_features, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "_TODO_: \n",
    "* this notebook must only load one dataset\n",
    "* retrieve dataset to load from cmd arguments or from env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.EGEOD22619.EGEOD22619Dataset import EGEOD22619Dataset\n",
    "from datasets.MILE.MileDataset import MileDataset\n",
    "from datasets.Golub99.GolubDataset import GolubDataset\n",
    "\n",
    "from datasets.DatasetEncoder import DatasetEncoder\n",
    "from datasets.DatasetSplitter import DatasetSplitter\n",
    "from datasets.DatasetLoader import DatasetLoader\n",
    "from datasets.DatasetBalancer import DatasetBalancer\n",
    "\n",
    "# Load dataset from environment variable. This is used by automated scripts\n",
    "ds_class = DatasetLoader.load_from_env_var(default_dataset=\"Golub\")\n",
    "\n",
    "print(\"Dataset used: %s\" % ds_class.__name__)\n",
    "\n",
    "ds = ds_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset transformation\n",
    "The dataset needs some transformations such as encoding the outputs as float (necessary for scikit learn), normalization, ...\n",
    "\n",
    "_TODO_:\n",
    "* dataset splitting (train, test[, validation])\n",
    "* encode outputs\n",
    "* normalization\n",
    "* classes merging\n",
    "    * due to the low class balancing we might want to regroup them. Example Healthy vs Non-Healthy (choose the most represented class ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode Dataset string classes into numbers\n",
    "ds_encoder = DatasetEncoder(ds)\n",
    "ds = ds_encoder.encode()\n",
    "\n",
    "ds_balancer = DatasetBalancer(ds)\n",
    "ds = ds_balancer.balance()\n",
    "\n",
    "ds = DatasetSplitter(ds, test_size=0.4)\n",
    "\n",
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "N_FEATURES = len(X_train[0])\n",
    "print(\"Number of genes: %d\" % N_FEATURES)\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_list_unique = len(merged_features) == len(set(merged_features))\n",
    "print(\"is list unique\", is_list_unique)\n",
    "\n",
    "merged_features = list(merged_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess merged features\n",
    "\n",
    "TODO: compare performance against a list of random features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(assessment_scores):\n",
    "    import math\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    n_subplots = len(assessment_scores)\n",
    "    cols = 3\n",
    "    rows = int(math.ceil(n_subplots / cols))\n",
    "    i = 1\n",
    "\n",
    "    for name, score_cm in assessment_scores.iteritems():\n",
    "        y_test, y_pred = score_cm[1]\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        plt.subplot(rows, cols, i)\n",
    "        i += 1\n",
    "\n",
    "        ConfusionMatrix.plot(cnf_matrix, classes=class_names,\n",
    "                              title='Confusion matrix for %s' % name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def assess_merged_features(clf, clf_name, selected_features, assessment_scores, verbose=True):\n",
    "    clf.fit(ds.get_X_train(), ds.get_y_train())\n",
    "    y_pred = clf.predict(ds.get_X_test())\n",
    "    y_test = ds.get_y_test()\n",
    "\n",
    "    scores = cross_val_score(clf, ds.get_X_test()[:, selected_features], y_test, cv=5, n_jobs=-1)\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    #if verbose:\n",
    "    #    print(\"[%s] Score using the merged list of features: %.3f\" % (clf_name, score))\n",
    "\n",
    "    assessment_scores[clf_name] = score, (y_test, y_pred)\n",
    "\n",
    "\n",
    "def assess_features(selected_features, verbose=True):\n",
    "    assessment_scores  = {}\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    assess_merged_features(clf, \"KNN\", selected_features, assessment_scores, verbose)\n",
    "\n",
    "    clf = MLPClassifier(solver=\"adam\", alpha=1e-3, hidden_layer_sizes=(100, 50), activation=\"relu\")\n",
    "    #clf = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', early_stopping=False,\n",
    "    #       epsilon=1e-08, hidden_layer_sizes=(100,50), learning_rate='constant',\n",
    "    #       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
    "    #       nesterovs_momentum=True, power_t=0.5, shuffle=True,\n",
    "    #       solver='lbfgs', tol=0.0001, verbose=False,\n",
    "    #       warm_start=False)\n",
    "\n",
    "    assess_merged_features(clf, \"MLP\", selected_features, assessment_scores, verbose)\n",
    "\n",
    "    clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n",
    "    assess_merged_features(clf, \"ExtraTrees\", selected_features, assessment_scores, verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        show_confusion_matrix(assessment_scores)\n",
    "    \n",
    "    # mean score\n",
    "    score = np.median([ass[0] for ass in assessment_scores.values()])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging techniques score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_index = 2\n",
    "\n",
    "for i, m_list in enumerate(merged_features_lists):\n",
    "    merging_technique, selected_features, score = m_list\n",
    "    selected_features = list(selected_features)\n",
    "    score = assess_features(selected_features)\n",
    "    print(\"---> [%s] mean score: %.3f\" % (merging_technique, score))\n",
    "\n",
    "    merged_features_lists[i] = (merging_technique, selected_features, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the merged techniques against k random features and against all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random features\n",
    "import random\n",
    "\n",
    "score = 0\n",
    "N = 10\n",
    "k = 100\n",
    "for _ in range(N):\n",
    "    random_features = random.sample(range(N_FEATURES), k)\n",
    "    score += assess_features(random_features, verbose=False)\n",
    "\n",
    "score = score/float(N)\n",
    "print(\"---> Random features scores: %.3f\" % score)\n",
    "\n",
    "#merging_techniques_score.append((\"%d random features\" % k, random_features, score))\n",
    "merged_features_lists.append((\"%d random features\" % k, random_features, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "\n",
    "all_features = range(N_FEATURES)\n",
    "score = assess_features(random_features, verbose=False)\n",
    "\n",
    "print(\"---> Using all features scores: %.3f\" % score)\n",
    "\n",
    "#merging_techniques_score.append((\"All features\", score))\n",
    "merged_features_lists.append((\"All features\", all_features, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a bar chart with the mean score for several merging methods\n",
    "\n",
    "# merging_techniques, scores = zip(*sorted(merging_techniques_score, key=lambda x : x[1], reverse=True))\n",
    "merging_techniques, merging_lists, scores = zip(*sorted(merged_features_lists, key=lambda x : x[score_index], reverse=True))\n",
    "\n",
    "y_pos = np.arange(len(merging_techniques))\n",
    "\n",
    "fig = plt.figure(figsize=(14,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(y_pos, scores, align='center', alpha=0.5, width=0.3)\n",
    "plt.xticks(y_pos, merging_techniques)\n",
    "\n",
    "# add values above the bars\n",
    "for a,b in enumerate(scores):\n",
    "    plt.text(a, b, \"%.3f\" % b, ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.title('Mean score between several merging methods')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
