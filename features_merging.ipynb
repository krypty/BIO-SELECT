{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Features merging using several lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here + highlight this notebook on the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "#np.random.seed(4)\n",
    "\n",
    "# increase font size in matplotlib\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Golub\n",
    "GROUP_NAME = \"golub_19122016\"\n",
    "DATASET = \"Golub\" # choose between \"Golub\" and \"MILE\"\n",
    "\n",
    "# Use MILE\n",
    "#GROUP_NAME = \"mile_19122016\"\n",
    "#DATASET = \"MILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: load the features lists from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.CSVFeaturesImporter import CSVFeaturesImporter\n",
    "\n",
    "importer = CSVFeaturesImporter(GROUP_NAME)\n",
    "subsets = importer.load()\n",
    "#print(subsets[\"features\"].keys())\n",
    "#print(subsets[\"features_by_score\"][\"ReliefF\"][:5])\n",
    "\n",
    "\n",
    "features = subsets[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.SimilarityMatrix import SimilarityMatrix\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "\n",
    "# plot the similarity matrices\n",
    "alg_names, features_subsets = zip(*features.items())\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=jaccard, \n",
    "                      title=\"Jaccard similarity between two feature subsets\")\n",
    "sm.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=intersection_count, \n",
    "                      title=\"Intersection between two feature subsets\")\n",
    "sm.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram - visualizing the \"distance\" between the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_names, f_values = zip(*subsets[\"features\"].items())\n",
    "\n",
    "# only keep the features indices, drop the features occurences\n",
    "def extract_lists(f_values):\n",
    "    for fv in f_values:\n",
    "        try:\n",
    "            yield [f_idx for f_idx, _ in fv]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "f_values = [i for i in extract_lists(f_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.Dendrogram import Dendrogram\n",
    "\n",
    "metrics = [\n",
    "    'rogerstanimoto',\n",
    "    'jaccard',\n",
    "    'dice',\n",
    "    'russellrao',\n",
    "    'yule'\n",
    "]\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    d = Dendrogram(lists=f_values, lists_labels=f_names, metric=m)\n",
    "    d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the lists of F Value and Fisher Score are the same (like the similarity matrix has shown).\n",
    "\n",
    "__For Golub only:__\n",
    "\n",
    "All the features in CFS are in MRMR (see the intersection in the similarity matrix). But CFS only contains 9 features in total. So the mask of features for CFS is almost a list of False values which means that the distance to the other lists (including MRMR) is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ensure that when we merge the lists of features, the list remains composed of unique features\n",
    "def assert_list_contains_only_unique_features(features):\n",
    "    assert len(features) == len(set(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# technique name, selected features\n",
    "merged_features_lists = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.UnionSubsetMerger import UnionSubsetMerger\n",
    "\n",
    "susm = UnionSubsetMerger(features_subsets)\n",
    "merged_features = susm.merge()\n",
    "\n",
    "merged_features_lists[\"Union of all features\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep top N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def group_by_features(features):\n",
    "    from itertools import groupby\n",
    "    \n",
    "    def keyfunc(x): return x[0]\n",
    "    \n",
    "    list_of_lists_sorted = sorted(features, key=keyfunc)\n",
    "    grouped_list = [list(j) for i, j in groupby(list_of_lists_sorted, key=keyfunc)]\n",
    "    return grouped_list\n",
    "\n",
    "def mean_score_for_feature(a, n_algorithms):\n",
    "    feat_name, feat_scores = zip(*a)\n",
    "    feat_name = feat_name[0] # since name is the same for all tuples\n",
    "    \n",
    "    m = sum(feat_scores)/float(n_algorithms)\n",
    "    return (feat_name, m)\n",
    "    \n",
    "def keep_top_n(features, n):\n",
    "    n_algorithms = len(features)\n",
    "    \n",
    "    all_feats = []\n",
    "    for f in features.values():\n",
    "        all_feats.extend(f)\n",
    "    \n",
    "    \n",
    "    print(all_feats[:8])\n",
    "    grouped_list = group_by_features(all_feats)\n",
    "    grouped_list = [mean_score_for_feature(f, n_algorithms) for f in grouped_list]\n",
    "    \n",
    "    grouped_list = sorted(grouped_list, key=lambda x: x[1], reverse=True)\n",
    "    print(grouped_list[:8])\n",
    "    \n",
    "    return [x[0] for x in grouped_list[:n]]\n",
    "    \n",
    "    \n",
    "merged_features = keep_top_n(subsets[\"features\"], n=100)\n",
    "merged_features_lists[\"Keep Top N features\"] = merged_features\n",
    "\n",
    "assert_list_contains_only_unique_features(merged_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union of intersection (two by two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def union_of_intersection_two_by_two(features):\n",
    "    sort_by_len_features = sorted(features.values(), key=lambda x:len(x), reverse=True)\n",
    "    print([len(f) for f in sort_by_len_features])\n",
    "    \n",
    "    def inter(x, y):\n",
    "        intersection = list(set(x).intersection(set(y)))\n",
    "        print(\"Intersection length : %d\" % len(intersection))\n",
    "        return intersection\n",
    "    \n",
    "    lists_of_features = [([a[0] for a in f]) for f in sort_by_len_features]\n",
    "    \n",
    "    # keep the lists that contains at least 500 features\n",
    "    lists_of_features = filter(lambda x:len(x) > 500, lists_of_features)\n",
    "    \n",
    "    return reduce(inter, lists_of_features)\n",
    "\n",
    "    \n",
    "merged_features = union_of_intersection_two_by_two(subsets[\"features_by_score\"])\n",
    "merged_features_lists[\"Union of intersections\"] = merged_features\n",
    "\n",
    "assert_list_contains_only_unique_features(merged_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "_TODO_: \n",
    "* this notebook must only load one dataset\n",
    "* retrieve dataset to load from cmd arguments or from env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.EGEOD22619.EGEOD22619Dataset import EGEOD22619Dataset\n",
    "from datasets.MILE.MileDataset import MileDataset\n",
    "from datasets.Golub99.GolubDataset import GolubDataset\n",
    "\n",
    "from datasets.DatasetEncoder import DatasetEncoder\n",
    "from datasets.DatasetSplitter import DatasetSplitter\n",
    "from datasets.DatasetLoader import DatasetLoader\n",
    "from datasets.DatasetBalancer import DatasetBalancer\n",
    "\n",
    "# Load dataset from environment variable. This is used by automated scripts\n",
    "ds_class = DatasetLoader.load_from_env_var(default_dataset=DATASET)\n",
    "\n",
    "print(\"Dataset used: %s\" % ds_class.__name__)\n",
    "\n",
    "ds = ds_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset transformation\n",
    "The dataset needs some transformations such as encoding the outputs as float (necessary for scikit learn), normalization, ...\n",
    "\n",
    "_TODO_:\n",
    "* dataset splitting (train, test[, validation])\n",
    "* encode outputs\n",
    "* normalization\n",
    "* classes merging\n",
    "    * due to the low class balancing we might want to regroup them. Example Healthy vs Non-Healthy (choose the most represented class ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode Dataset string classes into numbers\n",
    "ds_encoder = DatasetEncoder(ds)\n",
    "ds = ds_encoder.encode()\n",
    "\n",
    "ds = DatasetSplitter(ds, test_size=0.4)\n",
    "\n",
    "ds_balancer = DatasetBalancer(ds)\n",
    "ds = ds_balancer.balance()\n",
    "\n",
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "N_FEATURES = len(X_train[0])\n",
    "print(\"Number of genes: %d\" % N_FEATURES)\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess merged features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging techniques score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name, selected_features, score, std\n",
    "assessed_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from merge.SubsetAssessor import SubsetAssessor\n",
    "\n",
    "score_index = 2\n",
    "\n",
    "for m_technique_name, m_selected_features in merged_features_lists.iteritems():\n",
    "    m_selected_features = list(m_selected_features)\n",
    "    \n",
    "    sa = SubsetAssessor(m_selected_features, ds, k=10)\n",
    "    \n",
    "    score, std = sa.score, sa.std\n",
    "    print(\"[%s] median score: %.3f\" % (m_technique_name, score))\n",
    "\n",
    "    assessed_lists.append((m_technique_name, m_selected_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the merged techniques against k random features and against all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against random lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "score_std = []\n",
    "N = 5\n",
    "k = 100 # length of the random lists\n",
    "for _ in range(N):\n",
    "    random_features = random.sample(range(N_FEATURES), k)\n",
    "    sa = SubsetAssessor(random_features, ds, k=5)\n",
    "    score_std.append((sa.score, sa.std))\n",
    "\n",
    "\n",
    "# get the median of the scores. Warning: This is not the real median. \n",
    "# The real one would take the mean between the n/2 and (n/2)+1 elements if the n is even\n",
    "score, std = sorted(score_std, key=lambda x:x[0])[len(score_std)//2]\n",
    "print(\"Random features scores: %.3f\" % score)\n",
    "\n",
    "assessed_lists.append((\"%d random features\" % k, random_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = range(N_FEATURES)\n",
    "sa = SubsetAssessor(all_features, ds, k=10)\n",
    "score, std = sa.score, sa.std\n",
    "\n",
    "print(\"Using all features scores: %.3f\" % score)\n",
    "\n",
    "assessed_lists.append((\"All features\", all_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a bar chart with the mean score for the merging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_barchart_merging_methods(labels, scores, stds):\n",
    "    y_pos = np.arange(len(labels))\n",
    "\n",
    "    fig = plt.figure(figsize=(11,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.bar(y_pos, scores, align='center', yerr=stds, \n",
    "           alpha=0.8, width=0.3, color=\"turquoise\", edgecolor=\"turquoise\", ecolor=\"salmon\")\n",
    "\n",
    "    plt.xticks(y_pos, labels)\n",
    "\n",
    "    # add values above the bars\n",
    "    for a,b in enumerate(scores):\n",
    "        plt.text(a, b, \" %.3f\" % b, ha='left', va='bottom')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.title('Median score between several merging methods')\n",
    "    plt.gca().yaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "assessed_lists = sorted(assessed_lists, key=lambda x:x[score_index], reverse=True)\n",
    "names, selected_features, scores, stds = zip(*assessed_lists)\n",
    "\n",
    "labels = [\"%s\\n(#%d)\" % (name, len(feats)) for name, feats in zip(names, selected_features)]\n",
    "show_barchart_merging_methods(labels, scores, stds)\n",
    "\n",
    "print(stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
