{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Features merging using several lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/project_pipeline.png\",width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "#np.random.seed(4)\n",
    "\n",
    "# increase font size in matplotlib\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Golub\n",
    "#GROUP_NAME = \"golub_19122016\"\n",
    "#GROUP_NAME = \"golub_06022017\"\n",
    "#DATASET = \"Golub\" # choose between \"Golub\" and \"MILE\"\n",
    "\n",
    "# Use MILE\n",
    "#GROUP_NAME = \"MILE_21012017\"\n",
    "#DATASET = \"MILE\"\n",
    "\n",
    "GROUP_NAME = \"nci60_09052017\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.CSVFeaturesImporter import CSVFeaturesImporter\n",
    "\n",
    "importer = CSVFeaturesImporter(GROUP_NAME)\n",
    "subsets = importer.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the top 15 ranked features for each algorithm\n",
    "for name, values in subsets[\"features_by_rank\"].items():\n",
    "    best_ranked_feats, _ = zip(*values[:15])\n",
    "    print(\"%-15s%s\" % (name, best_ranked_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scores_per_features(scores_per_features, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the averaged score per features. The y axis is the average score of the feature and the x axis show the \n",
    "    features with the first one as the best ranked.\n",
    "    \n",
    "    Assumption: scores_per_features is sorted with the best ranked feature at index 0\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    features, scores = zip(*scores_per_features)\n",
    "    \n",
    "    xs = range(len(scores))\n",
    "    ys = scores\n",
    "    \n",
    "    #ax.bar(xs, ys, align='center', width=0.5, alpha=0.3)\n",
    "    ax.plot(xs,ys)\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Feature rank')\n",
    "    \n",
    "    plt.ylim(0, None)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    if title != \"\":\n",
    "        title = \"[\" + title + \"]\"\n",
    "    plt.title(\"%s Averaged score per features\" % title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s_name, s_values in subsets[\"features_by_score\"].items():\n",
    "    plot_scores_per_features(s_values, title=\"%s\" % s_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.SimilarityMatrix import SimilarityMatrix\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "\n",
    "# plot the similarity matrices\n",
    "alg_names, features_subsets = subsets[\"features\"].keys(), subsets[\"features\"].values()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=jaccard, \n",
    "                      title=\"Jaccard similarity between two feature subsets\")\n",
    "sm.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=intersection_count, \n",
    "                      title=\"Intersection between two feature subsets\")\n",
    "sm.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram - visualizing the \"distance\" between the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_names, f_values = zip(*subsets[\"features\"].items())\n",
    "\n",
    "# only keep the features indices, drop the features occurences\n",
    "def extract_lists(f_values):\n",
    "    for fv in f_values:\n",
    "        try:\n",
    "            yield [f_idx for f_idx, _ in fv]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "f_values = [i for i in extract_lists(f_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.Dendrogram import Dendrogram\n",
    "\n",
    "metrics = [\n",
    "    'rogerstanimoto',\n",
    "    'jaccard',\n",
    "    'dice',\n",
    "    'russellrao',\n",
    "    'yule'\n",
    "]\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    d = Dendrogram(lists=f_values, lists_labels=f_names, metric=m)\n",
    "    d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of these dendrograms is available in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# technique name, selected features\n",
    "merged_features_lists = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.UnionSubsetMerger import UnionSubsetMerger\n",
    "\n",
    "susm = UnionSubsetMerger(subsets[\"features\"].values())\n",
    "merged_features = susm.merge()\n",
    "\n",
    "merged_features_lists[\"Union of all features\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep top N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TopNMerger import TopNMerger\n",
    "\n",
    "merged_features = TopNMerger(subsets[\"features\"].values(), n=100).merge()\n",
    "merged_features_lists[\"Keep Top N features\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two by two intersections\n",
    "Take the intersection between two lists then intersects the result with the next one and so for each remaining list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TwoByTwoIntersectionsMerger import TwoByTwoIntersectionsMerger\n",
    "    \n",
    "merged_features = TwoByTwoIntersectionsMerger(subsets[\"features\"]).merge()\n",
    "merged_features_lists[\"Two by Two\\n intersections\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same merging technique but using only the lists with a score. All the lists given by algorithms who does not provide scores are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TwoByTwoIntersectionsMerger import TwoByTwoIntersectionsMerger\n",
    "    \n",
    "merged_features = TwoByTwoIntersectionsMerger(subsets[\"features_by_score\"]).merge()\n",
    "merged_features_lists[\"Two by Two\\n intersections (score)\"] = merged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.UnionOfIntersectionsMerger import UnionOfIntersectionsMerger\n",
    "\n",
    "merged_features = UnionOfIntersectionsMerger(subsets[\"features\"]).merge()\n",
    "merged_features_lists[\"Union of intersections\"] = merged_features\n",
    "\n",
    "print(\"#features kept: %d \" % len(merged_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.WeightedListsMerger import *\n",
    "\n",
    "wlm = WeightedListsMerger(subsets[\"features_by_rank\"], max_features_to_keep=300)\n",
    "merged_features = wlm.merge()\n",
    "merged_features_lists[\"Weighted lists\"] = merged_features\n",
    "\n",
    "for name, w in wlm.get_W_per_list():\n",
    "    print(\"%s: %.3f\" % (name, w))\n",
    "    \n",
    "print(\"Kept %d features\" % len(merged_features))\n",
    "wlm.show_dendrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "We use the serialized dataset (pkl file) to keep the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the same dataset object that was used to generate the lists of features.\n",
    "We are doing this because we can use the same split. Otherwise, we have to split the dataset again which might lead to have 'already seen samples' in the test set which can be considered as cheating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# do not forget to make sure that the dataset is not compressed (GROUP_NAME.pkl.tar.gz). Extract it if so.\n",
    "ds = pickle.load(open(\"%s.pkl\" % GROUP_NAME,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "N_FEATURES = len(X_train[0])\n",
    "print(\"Number of genes: %d\" % N_FEATURES)\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(y_test)\n",
    "print([\"class %d has %d samples\" % (c,s) for c, s in c.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess merged features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging techniques score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The used score function is F1-Score. This function can leads to 0/0 division.\n",
    "# Theses following lines hide warnings about 0/0 divisions when computing the F-Score. \n",
    "# When looking at the source code, all 0/0 divisions are set to 0. \n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name, selected_features, score, std\n",
    "assessed_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from merge.SubsetAssessor import SubsetAssessor\n",
    "\n",
    "score_index = 2\n",
    "\n",
    "for m_technique_name, m_selected_features in merged_features_lists.iteritems():\n",
    "    m_selected_features = list(m_selected_features)\n",
    "    \n",
    "    if len(m_selected_features) == 0:\n",
    "        print(\"[warning] %s technique was ignored because it contains 0 features\" % m_technique_name)\n",
    "        assessed_lists.append((m_technique_name, m_selected_features, 0, 0))\n",
    "        continue\n",
    "    \n",
    "    sa = SubsetAssessor(m_selected_features, ds, k=5)\n",
    "    \n",
    "    score, std = sa.score, sa.std\n",
    "    print(\"[%s] median score: %.2f\" % (m_technique_name, score))\n",
    "\n",
    "    assessed_lists.append((m_technique_name, m_selected_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the merged techniques against k random features and against all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against random lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "score_std = []\n",
    "N = 8\n",
    "k = 100 # length of the random lists\n",
    "for _ in range(N):\n",
    "    random_features = random.sample(range(N_FEATURES), k)\n",
    "    sa = SubsetAssessor(random_features, ds, k=5)\n",
    "    score_std.append((sa.score, sa.std))\n",
    "\n",
    "\n",
    "# get the median of the scores. Warning: This is not the real median. \n",
    "# The real one would take the mean between the n/2 and (n/2)+1 elements if the n is even\n",
    "score, std = sorted(score_std, key=lambda x:x[0])[len(score_std)//2]\n",
    "print(\"Random features scores: %.2f\" % score)\n",
    "\n",
    "assessed_lists.append((\"%d random features\" % k, random_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = range(N_FEATURES)\n",
    "sa = SubsetAssessor(all_features, ds, k=5)\n",
    "score, std = sa.score, sa.std\n",
    "\n",
    "print(\"Using all features scores: %.2f\" % score)\n",
    "\n",
    "assessed_lists.append((\"All features\", all_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a bar chart with the mean score for the merging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_barchart_merging_methods(labels, scores, stds):\n",
    "    y_pos = np.arange(len(labels))\n",
    "\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.bar(y_pos, scores, align='center', yerr=stds, \n",
    "           alpha=0.8, width=0.3, color=\"turquoise\", edgecolor=\"turquoise\", ecolor=\"black\")\n",
    "\n",
    "    plt.xticks(y_pos, labels)\n",
    "\n",
    "    # add values above the bars\n",
    "    for a,b in enumerate(scores):\n",
    "        plt.text(a, b, \" %.2f\" % b, ha='left', va='bottom')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.title('Median score between several merging methods')\n",
    "    plt.gca().yaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "assessed_lists = sorted(assessed_lists, key=lambda x:x[score_index], reverse=True)\n",
    "names, selected_features, scores, stds = zip(*assessed_lists)\n",
    "\n",
    "labels = [\"%s\\n(#%d)\" % (name, len(feats)) for name, feats in zip(names, selected_features)]\n",
    "show_barchart_merging_methods(labels, scores, stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
