{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Features merging using several lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here + highlight this notebook on the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "#np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Golub\n",
    "GROUP_NAME = \"golub_19122016\"\n",
    "DATASET = \"Golub\" # choose between \"Golub\" and \"MILE\"\n",
    "\n",
    "# Use MILE\n",
    "#GROUP_NAME = \"mile_19122016\"\n",
    "#DATASET = \"MILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: load the features lists from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.CSVFeaturesImporter import CSVFeaturesImporter\n",
    "\n",
    "importer = CSVFeaturesImporter(GROUP_NAME)\n",
    "subsets = importer.load()\n",
    "#print(subsets[\"features\"].keys())\n",
    "#print(subsets[\"features_by_score\"][\"ReliefF\"][:5])\n",
    "\n",
    "\n",
    "features = subsets[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.SimilarityMatrix import SimilarityMatrix\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "\n",
    "# plot the similarity matrices\n",
    "alg_names, features_subsets = zip(*features.items())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=jaccard, \n",
    "                      title=\"Jaccard similarity between two feature subsets\")\n",
    "sm.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=intersection_count, \n",
    "                      title=\"Intersection between two feature subsets\")\n",
    "sm.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram - visualizing the \"distance\" between the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_names, f_values = zip(*subsets[\"features\"].items())\n",
    "\n",
    "# only keep the features indices, drop the features occurences\n",
    "def extract_lists(f_values):\n",
    "    for fv in f_values:\n",
    "        try:\n",
    "            yield [f_idx for f_idx, _ in fv]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "f_values = [i for i in extract_lists(f_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.Dendrogram import Dendrogram\n",
    "\n",
    "metrics = [\n",
    "    'rogerstanimoto',\n",
    "    'jaccard',\n",
    "    'dice',\n",
    "    'russellrao',\n",
    "    'yule'\n",
    "]\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    d = Dendrogram(lists=f_values, lists_labels=f_names, metric=m)\n",
    "    d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the lists of F Value and Fisher Score are the same (like the similarity matrix has shown).\n",
    "\n",
    "__For Golub only:__\n",
    "\n",
    "All the features in CFS are in MRMR (see the intersection in the similarity matrix). But CFS only contains 9 features in total. So the mask of features for CFS is almost a list of False values which means that the distance to the other lists (including MRMR) is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ensure that when we merge the lists of features, the list remains composed of unique features\n",
    "def assert_list_contains_only_unique_features(features):\n",
    "    assert len(features) == len(set(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_features_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.simple.SimpleUnionSubsetMerger import SimpleUnionSubsetMerger\n",
    "\n",
    "susm = SimpleUnionSubsetMerger(features_subsets)\n",
    "merged_features = susm.merge()\n",
    "n_all_features = sum([len(s) for s in features_subsets])\n",
    "\n",
    "print(\"Unique features (union of all subsets): %d over a total of %d \" % (len(merged_features), n_all_features))\n",
    "\n",
    "merged_features_lists.append((\"Union of all features\", merged_features, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subsets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def group_by_features(features):\n",
    "    from itertools import groupby\n",
    "    \n",
    "    def keyfunc(x): return x[0]\n",
    "    \n",
    "    list_of_lists_sorted = sorted(features, key=keyfunc)\n",
    "    grouped_list = [list(j) for i, j in groupby(list_of_lists_sorted, key=keyfunc)]\n",
    "    return grouped_list\n",
    "\n",
    "def mean_score_for_feature(a, n_algorithms):\n",
    "    feat_name, feat_scores = zip(*a)\n",
    "    feat_name = feat_name[0] # since name is the same for all tuples\n",
    "    \n",
    "    m = sum(feat_scores)/float(n_algorithms)\n",
    "    return (feat_name, m)\n",
    "    \n",
    "def keep_top_n(features, n):\n",
    "    n_algorithms = len(features)\n",
    "    \n",
    "    all_feats = []\n",
    "    for f in features.values():\n",
    "        all_feats.extend(f)\n",
    "    \n",
    "    \n",
    "    print(all_feats[:8])\n",
    "    grouped_list = group_by_features(all_feats)\n",
    "    grouped_list = [mean_score_for_feature(f, n_algorithms) for f in grouped_list]\n",
    "    \n",
    "    grouped_list = sorted(grouped_list, key=lambda x: x[1], reverse=True)\n",
    "    print(grouped_list[:8])\n",
    "    \n",
    "    return [x[0] for x in grouped_list[:n]]\n",
    "    \n",
    "    \n",
    "merged_features = keep_top_n(subsets[\"features\"], n=100)\n",
    "merged_features_lists.append((\"Keep Top N features\", merged_features, -1))\n",
    "\n",
    "assert_list_contains_only_unique_features(merged_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def union_of_intersection_two_by_two(features):\n",
    "    sort_by_len_features = sorted(features.values(), key=lambda x:len(x), reverse=True)\n",
    "    print([len(f) for f in sort_by_len_features])\n",
    "    \n",
    "    def inter(x, y):\n",
    "        intersection = list(set(x).intersection(set(y)))\n",
    "        print(\"Intersection length : %d\" % len(intersection))\n",
    "        return intersection\n",
    "    \n",
    "    lists_of_features = [([a[0] for a in f]) for f in sort_by_len_features]\n",
    "    \n",
    "    # keep the lists that contains at least 500 features\n",
    "    lists_of_features = filter(lambda x:len(x) > 500, lists_of_features)\n",
    "    \n",
    "    return reduce(inter, lists_of_features)\n",
    "\n",
    "    \n",
    "merged_features = union_of_intersection_two_by_two(subsets[\"features_by_score\"])\n",
    "merged_features_lists.append((\"Union of intersections\", merged_features, -1))\n",
    "\n",
    "assert_list_contains_only_unique_features(merged_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "_TODO_: \n",
    "* this notebook must only load one dataset\n",
    "* retrieve dataset to load from cmd arguments or from env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.EGEOD22619.EGEOD22619Dataset import EGEOD22619Dataset\n",
    "from datasets.MILE.MileDataset import MileDataset\n",
    "from datasets.Golub99.GolubDataset import GolubDataset\n",
    "\n",
    "from datasets.DatasetEncoder import DatasetEncoder\n",
    "from datasets.DatasetSplitter import DatasetSplitter\n",
    "from datasets.DatasetLoader import DatasetLoader\n",
    "from datasets.DatasetBalancer import DatasetBalancer\n",
    "\n",
    "# Load dataset from environment variable. This is used by automated scripts\n",
    "ds_class = DatasetLoader.load_from_env_var(default_dataset=DATASET)\n",
    "\n",
    "print(\"Dataset used: %s\" % ds_class.__name__)\n",
    "\n",
    "ds = ds_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset transformation\n",
    "The dataset needs some transformations such as encoding the outputs as float (necessary for scikit learn), normalization, ...\n",
    "\n",
    "_TODO_:\n",
    "* dataset splitting (train, test[, validation])\n",
    "* encode outputs\n",
    "* normalization\n",
    "* classes merging\n",
    "    * due to the low class balancing we might want to regroup them. Example Healthy vs Non-Healthy (choose the most represented class ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode Dataset string classes into numbers\n",
    "ds_encoder = DatasetEncoder(ds)\n",
    "ds = ds_encoder.encode()\n",
    "\n",
    "ds = DatasetSplitter(ds, test_size=0.4)\n",
    "\n",
    "ds_balancer = DatasetBalancer(ds)\n",
    "ds = ds_balancer.balance()\n",
    "\n",
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "N_FEATURES = len(X_train[0])\n",
    "print(\"Number of genes: %d\" % N_FEATURES)\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess merged features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(assessment_scores):\n",
    "    import math\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    n_subplots = len(assessment_scores)\n",
    "    cols = 3\n",
    "    rows = int(math.ceil(n_subplots / cols))\n",
    "    i = 1\n",
    "\n",
    "    for name, score_cm in assessment_scores.iteritems():\n",
    "        y_test, y_pred = score_cm[1]\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        plt.subplot(rows, cols, i)\n",
    "        i += 1\n",
    "\n",
    "        ConfusionMatrix.plot(cnf_matrix, classes=class_names,\n",
    "                              title='Confusion matrix for %s' % name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def assess_merged_features(clf, clf_name, selected_features, assessment_scores, verbose=True):\n",
    "    # fixme: cross val on all the dataset ? remove the fit and confusion matrices ?\n",
    "    \n",
    "    clf.fit(ds.get_X_train(), ds.get_y_train())\n",
    "    y_pred = clf.predict(ds.get_X_test())\n",
    "    y_test = ds.get_y_test()\n",
    "\n",
    "    scores = cross_val_score(clf, ds.get_X_test()[:, selected_features], y_test, cv=5, n_jobs=-1)\n",
    "    score = np.mean(scores)\n",
    "    # todo: compute std\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[%s] Score using the merged list of features: %.3f\" % (clf_name, score))\n",
    "\n",
    "    assessment_scores[clf_name] = score, (y_test, y_pred)\n",
    "\n",
    "\n",
    "def assess_features(selected_features, verbose=True):\n",
    "    assessment_scores  = {}\n",
    "\n",
    "    clf = KNeighborsClassifier(algorithm=\"ball_tree\", n_neighbors=5, n_jobs=-1, metric=\"manhattan\")\n",
    "    assess_merged_features(clf, \"KNN\", selected_features, assessment_scores, verbose)\n",
    "\n",
    "    clf = MLPClassifier(solver=\"adam\", alpha=1e-3, hidden_layer_sizes=(100, 50), activation=\"relu\")\n",
    "    assess_merged_features(clf, \"MLP\", selected_features, assessment_scores, verbose)\n",
    "\n",
    "    clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n",
    "    assess_merged_features(clf, \"ExtraTrees\", selected_features, assessment_scores, verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        show_confusion_matrix(assessment_scores)\n",
    "    \n",
    "    # mean score\n",
    "    score = np.median([ass[0] for ass in assessment_scores.values()])\n",
    "    std = np.std([ass[0] for ass in assessment_scores.values()])\n",
    "    return score, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging techniques score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_index = 2\n",
    "\n",
    "for i, m_list in enumerate(merged_features_lists):\n",
    "    merging_technique, selected_features, score = m_list\n",
    "    selected_features = list(selected_features)\n",
    "    score, std = assess_features(selected_features)\n",
    "    print(\"---> [%s] median score: %.3f\" % (merging_technique, score))\n",
    "\n",
    "    merged_features_lists[i] = (merging_technique, selected_features, score, std)\n",
    "    \n",
    "    # add space for readability\n",
    "    print(\"\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the merged techniques against k random features and against all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random features\n",
    "import random\n",
    "\n",
    "score = 0\n",
    "std = 0\n",
    "N = 7\n",
    "k = 100 # length of the random lists\n",
    "for _ in range(N):\n",
    "    random_features = random.sample(range(N_FEATURES), k)\n",
    "    score_tmp, std_tmp = assess_features(random_features, verbose=False)\n",
    "    score += score_tmp\n",
    "    std += std_tmp\n",
    "\n",
    "score = score/float(N)\n",
    "std = std/float(N)\n",
    "print(\"---> Random features scores: %.3f\" % score)\n",
    "\n",
    "merged_features_lists.append((\"%d random features\" % k, random_features, score, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "\n",
    "all_features = range(N_FEATURES)\n",
    "score, std = assess_features(random_features, verbose=True)\n",
    "\n",
    "print(\"---> Using all features scores: %.3f\" % score)\n",
    "\n",
    "merged_features_lists.append((\"All features\", all_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a bar chart with the mean score for the merging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merging_techniques, merging_lists, scores, stds = zip(*sorted(merged_features_lists, key=lambda x : x[score_index], reverse=True))\n",
    "\n",
    "y_pos = np.arange(len(merging_techniques))\n",
    "\n",
    "fig = plt.figure(figsize=(11,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(y_pos, scores, align='center', yerr=stds, \n",
    "       alpha=0.8, width=0.3, color=\"turquoise\", edgecolor=\"turquoise\", ecolor=\"salmon\")\n",
    "\n",
    "merging_techniques_labels = [\"%s\\n(#%d)\" % (m_name, len(m_list)) for m_name, m_list in zip(merging_techniques, merging_lists)]\n",
    "plt.xticks(y_pos, merging_techniques_labels)\n",
    "\n",
    "# add values above the bars\n",
    "for a,b in enumerate(scores):\n",
    "    plt.text(a, b, \" %.3f\" % b, ha='left', va='bottom')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.title('Median score between several merging methods')\n",
    "plt.gca().yaxis.grid(True)\n",
    "plt.tight_layout()\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
