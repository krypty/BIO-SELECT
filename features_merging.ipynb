{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Features merging using several lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here + highlight this notebook on the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "#np.random.seed(4)\n",
    "\n",
    "# increase font size in matplotlib\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Golub\n",
    "#GROUP_NAME = \"golub_19122016\"\n",
    "GROUP_NAME = \"golub_16012017\"\n",
    "DATASET = \"Golub\" # choose between \"Golub\" and \"MILE\"\n",
    "\n",
    "# Use MILE\n",
    "#GROUP_NAME = \"MILE_21012017\"\n",
    "#DATASET = \"MILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: load the features lists from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.CSVFeaturesImporter import CSVFeaturesImporter\n",
    "\n",
    "importer = CSVFeaturesImporter(GROUP_NAME)\n",
    "subsets = importer.load()\n",
    "#print(subsets[\"features\"].keys())\n",
    "#print(subsets[\"features_by_score\"][\"ReliefF\"][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.SimilarityMatrix import SimilarityMatrix\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "\n",
    "# plot the similarity matrices\n",
    "alg_names, features_subsets = subsets[\"features\"].keys(), subsets[\"features\"].values()\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=jaccard, \n",
    "                      title=\"Jaccard similarity between two feature subsets\")\n",
    "sm.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sm = SimilarityMatrix(features_subsets, alg_names, compare_func=intersection_count, \n",
    "                      title=\"Intersection between two feature subsets\")\n",
    "sm.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram - visualizing the \"distance\" between the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_names, f_values = zip(*subsets[\"features\"].items())\n",
    "\n",
    "# only keep the features indices, drop the features occurences\n",
    "def extract_lists(f_values):\n",
    "    for fv in f_values:\n",
    "        try:\n",
    "            yield [f_idx for f_idx, _ in fv]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "f_values = [i for i in extract_lists(f_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.Dendrogram import Dendrogram\n",
    "\n",
    "metrics = [\n",
    "    'rogerstanimoto',\n",
    "    'jaccard',\n",
    "    'dice',\n",
    "    'russellrao',\n",
    "    'yule'\n",
    "]\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    d = Dendrogram(lists=f_values, lists_labels=f_names, metric=m)\n",
    "    d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the lists of F Value and Fisher Score are the same (like the similarity matrix has shown).\n",
    "\n",
    "__For Golub only:__\n",
    "\n",
    "All the features in CFS are in MRMR (see the intersection in the similarity matrix). But CFS only contains 9 features in total. So the mask of features for CFS is almost a list of False values which means that the distance to the other lists (including MRMR) is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# technique name, selected features\n",
    "merged_features_lists = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.UnionSubsetMerger import UnionSubsetMerger\n",
    "\n",
    "susm = UnionSubsetMerger(subsets[\"features\"].values())\n",
    "merged_features = susm.merge()\n",
    "\n",
    "merged_features_lists[\"Union of all features\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep top N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TopNMerger import TopNMerger\n",
    "  \n",
    "merged_features = TopNMerger(subsets[\"features\"].values(), n=100).merge()\n",
    "merged_features_lists[\"Keep Top N features\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two by two intersections\n",
    "Take the intersection between two lists then intersects the result with the next one and so for each remaining list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TwoByTwoIntersectionsMerger import TwoByTwoIntersectionsMerger\n",
    "    \n",
    "merged_features = TwoByTwoIntersectionsMerger(subsets[\"features\"]).merge()\n",
    "merged_features_lists[\"Two by Two\\n intersections\"] = merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same merging technique but using only the lists with a score. All the lists given by algorithms who does not provide scores are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.TwoByTwoIntersectionsMerger import TwoByTwoIntersectionsMerger\n",
    "    \n",
    "merged_features = TwoByTwoIntersectionsMerger(subsets[\"features_by_score\"]).merge()\n",
    "merged_features_lists[\"Two by Two\\n intersections (score)\"] = merged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.UnionOfIntersectionsMerger import UnionOfIntersectionsMerger\n",
    "\n",
    "merged_features = UnionOfIntersectionsMerger(subsets[\"features\"]).merge()\n",
    "merged_features_lists[\"Union of intersections\"] = merged_features\n",
    "\n",
    "print(\"#features kept: %d \" % len(merged_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.techniques.WeightedListsMerger import *\n",
    "\n",
    "wlm = WeightedListsMerger(subsets[\"features_by_rank\"], max_features_to_keep=300)\n",
    "merged_features = wlm.merge()\n",
    "merged_features_lists[\"Weighted lists\"] = merged_features\n",
    "\n",
    "for name, w in wlm.get_W_per_list():\n",
    "    print(\"%s: %.3f\" % (name, w))\n",
    "    \n",
    "print(\"Kept %d features\" % len(merged_features))\n",
    "wlm.show_dendrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the same dataset object that was used to generate the lists of features.\n",
    "We are doing this because we can use the same split. Otherwise, we have to split the dataset again which might lead to have 'already seen samples' in the test set which can be considered as cheating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ds = pickle.load(open(\"%s.pkl\" % GROUP_NAME,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "N_FEATURES = len(X_train[0])\n",
    "print(\"Number of genes: %d\" % N_FEATURES)\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(y_test)\n",
    "print([\"class %d has %d samples\" % (c,s) for c, s in c.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess merged features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging techniques score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The used score function is F1-Score. This function can leads to 0/0 division.\n",
    "# Theses following lines hide warnings about 0/0 divisions when computing the F-Score. \n",
    "# When looking at the source code, all 0/0 divisions are set to 0. \n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name, selected_features, score, std\n",
    "assessed_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from merge.SubsetAssessor import SubsetAssessor\n",
    "\n",
    "score_index = 2\n",
    "\n",
    "for m_technique_name, m_selected_features in merged_features_lists.iteritems():\n",
    "    m_selected_features = list(m_selected_features)\n",
    "    \n",
    "    if len(m_selected_features) == 0:\n",
    "        print(\"[warning] %s technique was ignored because it contains 0 features\" % m_technique_name)\n",
    "        assessed_lists.append((m_technique_name, m_selected_features, 0, 0))\n",
    "        continue\n",
    "    \n",
    "    sa = SubsetAssessor(m_selected_features, ds, k=5)\n",
    "    \n",
    "    score, std = sa.score, sa.std\n",
    "    print(\"[%s] median score: %.2f\" % (m_technique_name, score))\n",
    "\n",
    "    assessed_lists.append((m_technique_name, m_selected_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the merged techniques against k random features and against all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against random lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "score_std = []\n",
    "N = 8\n",
    "k = 100 # length of the random lists\n",
    "for _ in range(N):\n",
    "    random_features = random.sample(range(N_FEATURES), k)\n",
    "    sa = SubsetAssessor(random_features, ds, k=5)\n",
    "    score_std.append((sa.score, sa.std))\n",
    "\n",
    "\n",
    "# get the median of the scores. Warning: This is not the real median. \n",
    "# The real one would take the mean between the n/2 and (n/2)+1 elements if the n is even\n",
    "score, std = sorted(score_std, key=lambda x:x[0])[len(score_std)//2]\n",
    "print(\"Random features scores: %.2f\" % score)\n",
    "\n",
    "assessed_lists.append((\"%d random features\" % k, random_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = range(N_FEATURES)\n",
    "sa = SubsetAssessor(all_features, ds, k=5)\n",
    "score, std = sa.score, sa.std\n",
    "\n",
    "print(\"Using all features scores: %.2f\" % score)\n",
    "\n",
    "assessed_lists.append((\"All features\", all_features, score, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a bar chart with the mean score for the merging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_barchart_merging_methods(labels, scores, stds):\n",
    "    y_pos = np.arange(len(labels))\n",
    "\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.bar(y_pos, scores, align='center', yerr=stds, \n",
    "           alpha=0.8, width=0.3, color=\"turquoise\", edgecolor=\"turquoise\", ecolor=\"black\")\n",
    "\n",
    "    plt.xticks(y_pos, labels)\n",
    "\n",
    "    # add values above the bars\n",
    "    for a,b in enumerate(scores):\n",
    "        plt.text(a, b, \" %.2f\" % b, ha='left', va='bottom')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.title('Median score between several merging methods')\n",
    "    plt.gca().yaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "assessed_lists = sorted(assessed_lists, key=lambda x:x[score_index], reverse=True)\n",
    "names, selected_features, scores, stds = zip(*assessed_lists)\n",
    "\n",
    "labels = [\"%s\\n(#%d)\" % (name, len(feats)) for name, feats in zip(names, selected_features)]\n",
    "show_barchart_merging_methods(labels, scores, stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
