{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Global pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_FEATURES_ALGORITHM = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "_TODO_: \n",
    "* this notebook must only load one dataset\n",
    "* retrieve dataset to load from cmd arguments or from env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.EGEOD22619.EGEOD22619Dataset import EGEOD22619Dataset\n",
    "from datasets.MILE.MileDataset import MileDataset\n",
    "from datasets.Golub99.GolubDataset import GolubDataset\n",
    "\n",
    "from datasets.DatasetEncoder import DatasetEncoder\n",
    "from datasets.DatasetSplitter import DatasetSplitter\n",
    "\n",
    "#ds = MileDataset()\n",
    "#ds = EGEOD22619Dataset()\n",
    "ds = GolubDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset transformation\n",
    "The dataset needs some transformations such as encoding the outputs as float (necessary for scikit learn), normalization, ...\n",
    "\n",
    "_TODO_:\n",
    "* dataset splitting (train, test[, validation])\n",
    "* encode outputs\n",
    "* normalization\n",
    "* classes merging\n",
    "    * due to the low class balancing we might want to regroup them. Example Healthy vs Non-Healthy (choose the most represented class ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode Dataset string classes into numbers\n",
    "ds_encoder = DatasetEncoder(ds)\n",
    "ds = ds_encoder.encode()\n",
    "ds = DatasetSplitter(ds, test_size=0.4)\n",
    "\n",
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "print(len(y))\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "Run the chosen algorithms and save them and their output subset of features using cPickle into files. They can be used later to display some graphs and to be analyzed\n",
    "\n",
    "_TODO_: Write a subsection for each algorithm :\n",
    "* OneVsRest or OneVsOne ?\n",
    "    * only for those who needs it\n",
    "* Grid search + CV\n",
    "    * maybe not for all algorithms such as SVM RFE which takes a lot of time\n",
    "    * not for algorthms which does not have parameters to tune (ReliefF, Fisher Score,...)\n",
    "* print classification report (accuracy, recall, precision, ...)\n",
    "    * issue: not all algortihms are able to do this\n",
    "* normalize score using minmax normalization (0-1)\n",
    "* show score per features (50 to 100 first ones)\n",
    "* save algorithm in a file\n",
    "\n",
    "Algorithms:\n",
    "* ExtraTrees\n",
    "* Random Forest\n",
    "* SVM\n",
    "* SVM RFE\n",
    "* ANN\n",
    "* ReliefF\n",
    "* Fisher Score\n",
    "* \"Best features subset ~ SVM\"\n",
    "* SVM Backward ?\n",
    "* CFS - Correlation-based Feature Selection\n",
    "* Mutual Information Classifier\n",
    "* One genetic based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from algorithms.ExtraTreesAlgorithm import ExtraTreesAlgorithm\n",
    "from algorithms.ReliefFAlgorithm import ReliefFAlgorithm\n",
    "from algorithms.FisherScoreAlgorithm import FisherScoreAlgorithm\n",
    "from algorithms.FValueAlgorithm import FValueAlgorithm\n",
    "from algorithms.SVMAlgorithm import SVMAlgorithm\n",
    "\n",
    "eta_grid = [{'n_estimators': np.arange(10, 1000, 300), 'criterion': [\"gini\", \"entropy\"], 'max_features': [\"sqrt\", \"auto\", \"log2\"], \"n_jobs\": [-1]}]\n",
    "%time eta = ExtraTreesAlgorithm(ds, eta_grid)\n",
    "print(\"ExtraTrees best params \\n\\t%s\" % eta.best_params)\n",
    "\n",
    "\n",
    "rff = ReliefFAlgorithm(ds)\n",
    "fsa = FisherScoreAlgorithm(ds)\n",
    "fva = FValueAlgorithm(ds)\n",
    "\n",
    "#FIXME: grid search does not seem to work (for SVM at least)\n",
    "gridsearch_params = [{\n",
    "        'kernel':['linear'],\n",
    "        'C':[200, 0.1, 1, 10, 100, 1000],\n",
    "        'gamma' : [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'tol' : [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'cache_size':[1024]\n",
    "    }]\n",
    "%time svm_gs = SVMAlgorithm(ds, gridsearch_params)\n",
    "print(\"Best params \\n\\t%s\" % svm_gs.best_params)\n",
    "\n",
    "%time svm = SVMAlgorithm(ds)\n",
    "\n",
    "algorithms = [eta, rff, fsa, fva, svm_gs, svm]\n",
    "\n",
    "subsets = []\n",
    "alg_names = []\n",
    "for alg in algorithms:\n",
    "    feats_by_score = alg.get_best_features_by_score(n=N_FEATURES_ALGORITHM)\n",
    "    \n",
    "    subsets.append(feats_by_score)\n",
    "    alg_names.append(alg.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "        * Cosine similarity\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return 1.0 - spatial.distance.cosine(np.array(list(a)), np.array(list(b)))\n",
    "\n",
    "def compute_similary_between_subsets(subsets, compare_func):\n",
    "    N_subsets = len(subsets)\n",
    "    similarity_matrix = np.zeros(shape=(N_subsets, N_subsets))\n",
    "\n",
    "    for i, j in itertools.product(range(N_subsets), range(N_subsets)):\n",
    "        subset_i = {i[0] for i in subsets[i]}\n",
    "        subset_j = {j[0] for j in subsets[j]}\n",
    "        similarity_matrix[i, j] = compare_func(subset_i, subset_j)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "def plot_feature_subsets_matrix(cm, alg_names, title, cmap=plt.cm.Blues):\n",
    "    title += \"\\n\" # add a little margin for the title\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.colorbar()\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(alg_names))\n",
    "    plt.xticks(tick_marks, alg_names, rotation=45)\n",
    "    plt.yticks(tick_marks, alg_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 + 0.1\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        text = \"%.2f\" % cm[i, j]\n",
    "        plt.text(j, i, text,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 backgroundcolor=\"white\",\n",
    "                 #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                 color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=jaccard)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Jaccard similarity between two feature subsets\")\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=intersection_count)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Intersection between two feature subsets\")\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=cosine_similarity)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Cosine similarity between two features subsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.simple.SimpleUnionSubsetMerger import SimpleUnionSubsetMerger\n",
    "\n",
    "susm = SimpleUnionSubsetMerger(subsets)\n",
    "ensemble_set = susm.merge()\n",
    "\n",
    "print(\"Unique features (union of all subsets): %d over a total of %d \" % (len(ensemble_set), (N_FEATURES_ALGORITHM * len(subsets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
