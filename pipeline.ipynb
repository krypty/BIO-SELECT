{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BIO-SELECT - Marigliano\n",
    "## Global pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO_ : insert global pipeline image here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set float precision at 2 digits\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_FEATURES_ALGORITHM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "_TODO_: \n",
    "* this notebook must only load one dataset\n",
    "* retrieve dataset to load from cmd arguments or from env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.EGEOD22619.EGEOD22619Dataset import EGEOD22619Dataset\n",
    "from datasets.MILE.MileDataset import MileDataset\n",
    "from datasets.Golub99.GolubDataset import GolubDataset\n",
    "\n",
    "from datasets.DatasetEncoder import DatasetEncoder\n",
    "from datasets.DatasetSplitter import DatasetSplitter\n",
    "from datasets.DatasetLoader import DatasetLoader\n",
    "from datasets.DatasetBalancer import DatasetBalancer\n",
    "\n",
    "# Load dataset from environment variable. This is used by automated scripts\n",
    "ds_class = DatasetLoader.load_from_env_var(default_dataset=\"MILE\")\n",
    "\n",
    "print(\"Dataset used: %s\" % ds_class.__name__)\n",
    "\n",
    "ds = ds_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset transformation\n",
    "The dataset needs some transformations such as encoding the outputs as float (necessary for scikit learn), normalization, ...\n",
    "\n",
    "_TODO_:\n",
    "* dataset splitting (train, test[, validation])\n",
    "* encode outputs\n",
    "* normalization\n",
    "* classes merging\n",
    "    * due to the low class balancing we might want to regroup them. Example Healthy vs Non-Healthy (choose the most represented class ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode Dataset string classes into numbers\n",
    "ds_encoder = DatasetEncoder(ds)\n",
    "ds = ds_encoder.encode()\n",
    "\n",
    "ds_balancer = DatasetBalancer(ds)\n",
    "ds = ds_balancer.balance()\n",
    "\n",
    "ds = DatasetSplitter(ds, test_size=0.4)\n",
    "\n",
    "X = ds.get_X()\n",
    "y = ds.get_y()\n",
    "\n",
    "X_train = ds.get_X_train()\n",
    "y_train = ds.get_y_train()\n",
    "X_test = ds.get_X_test()\n",
    "y_test = ds.get_y_test()\n",
    "\n",
    "print(\"Number of genes: %d\" % len(X_train[0]))\n",
    "print(\"Dataset samples: %d\" % len(y))\n",
    "print(\"Train set size %d\" % len(X_train))\n",
    "print(\"Test set size %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "Run the chosen algorithms and save them and their output subset of features using cPickle into files. They can be used later to display some graphs and to be analyzed\n",
    "\n",
    "_TODO_: Write a subsection for each algorithm :\n",
    "* OneVsRest or OneVsOne ?\n",
    "    * only for those who needs it\n",
    "* Grid search + CV\n",
    "    * maybe not for all algorithms such as SVM RFE which takes a lot of time\n",
    "    * not for algorthms which does not have parameters to tune (ReliefF, Fisher Score,...)\n",
    "* print classification report (accuracy, recall, precision, ...)\n",
    "    * issue: not all algortihms are able to do this\n",
    "* normalize score using minmax normalization (0-1)\n",
    "* show score per features (50 to 100 first ones)\n",
    "* save algorithm in a file\n",
    "\n",
    "Algorithms:\n",
    "* ExtraTrees\n",
    "* Random Forest\n",
    "* SVM\n",
    "* SVM RFE\n",
    "* ANN\n",
    "* ReliefF\n",
    "* Fisher Score\n",
    "* \"Best features subset ~ SVM\"\n",
    "* SVM Backward ?\n",
    "* CFS - Correlation-based Feature Selection\n",
    "* Mutual Information Classifier\n",
    "* One genetic based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from algorithms.Algorithm import NotSupportedException\n",
    "from algorithms.ExtraTreesAlgorithm import ExtraTreesAlgorithm\n",
    "from algorithms.ReliefFAlgorithm import ReliefFAlgorithm\n",
    "from algorithms.FisherScoreAlgorithm import FisherScoreAlgorithm\n",
    "from algorithms.FValueAlgorithm import FValueAlgorithm\n",
    "from algorithms.SVMAlgorithm import SVMAlgorithm\n",
    "from algorithms.GAANNAlgorithm import GAANNAlgorithm\n",
    "\n",
    "algorithms = []\n",
    "\n",
    "eta_grid = [{'n_estimators': np.arange(10, 1000, 300), 'criterion': [\"gini\", \"entropy\"], 'max_features': [\"sqrt\", \"auto\", \"log2\"], \"n_jobs\": [-1]}]\n",
    "%time eta = ExtraTreesAlgorithm(ds, N_FEATURES_ALGORITHM, eta_grid)\n",
    "algorithms.append(eta)\n",
    "print(\"ExtraTrees best params \\n\\t%s\" % eta.best_params)\n",
    "\n",
    "\n",
    "rff = ReliefFAlgorithm(ds, N_FEATURES_ALGORITHM)\n",
    "algorithms.append(rff)\n",
    "\n",
    "fsa = FisherScoreAlgorithm(ds, N_FEATURES_ALGORITHM)\n",
    "algorithms.append(fsa)\n",
    "\n",
    "fva = FValueAlgorithm(ds, N_FEATURES_ALGORITHM)\n",
    "algorithms.append(fva)\n",
    "\n",
    "\n",
    "#FIXME: grid search does not seem to work (for SVM at least)\n",
    "gridsearch_params = [{\n",
    "        'kernel':['linear'],\n",
    "        'C':[200, 0.1, 1, 10, 100, 1000],\n",
    "        'gamma' : [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'tol' : [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'cache_size':[1024]\n",
    "    }]\n",
    "#%time svm_gs = SVMAlgorithm(ds, N_FEATURES_ALGORITHM, gridsearch_params)\n",
    "#algorithms.append(svm_gs)\n",
    "#print(\"Best params \\n\\t%s\" % svm_gs.best_params)\n",
    "\n",
    "%time svm = SVMAlgorithm(ds, N_FEATURES_ALGORITHM)\n",
    "algorithms.append(svm)\n",
    "\n",
    "#%time gaanna = GAANNAlgorithm(ds, N_FEATURES_ALGORITHM)\n",
    "#algorithms.append(gaanna)\n",
    "\n",
    "subsets = []\n",
    "alg_names = []\n",
    "for alg in algorithms:\n",
    "    feats = alg.get_best_features()\n",
    "    \n",
    "    subsets.append(feats)\n",
    "    alg_names.append(alg.name)\n",
    "    \n",
    "    try:\n",
    "        print(\"[%s] score: %.3f\" % (alg.name, alg.get_score()))\n",
    "    except NotSupportedException:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features subsets merging\n",
    "Each algorithm has done its work and provide a subset of features as:\n",
    "* a ranked score list\n",
    "* a ranked list (no score)\n",
    "* a list (no ranking, no score)\n",
    "\n",
    "This part uses some techniques to combine/merge theses lists into a better one\n",
    "\n",
    "_TODO_: \n",
    "* Visualize the lists\n",
    "    * Venn diagram ? --> limited to 3 sets, does not scale\n",
    "    * matrix: show the similarity of features between two subsets\n",
    "        * Jaccard\n",
    "        * Union\n",
    "        * Cosine similarity\n",
    "* implement merge techniques\n",
    "    * votation\n",
    "    * weighted votation\n",
    "    * union of intersection\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# some set similarity functions\n",
    "def intersection_count(a, b):\n",
    "    return len(a.intersection(b))\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b))/float(len(a.union(b)))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return 1.0 - spatial.distance.cosine(np.array(list(a)), np.array(list(b)))\n",
    "\n",
    "def compute_similary_between_subsets(subsets, compare_func):\n",
    "    N_subsets = len(subsets)\n",
    "    similarity_matrix = np.zeros(shape=(N_subsets, N_subsets))\n",
    "\n",
    "    for i, j in itertools.product(range(N_subsets), range(N_subsets)):\n",
    "        if isinstance(subsets[0][0], int):\n",
    "            subset_i = set(subsets[i])\n",
    "            subset_j = set(subsets[j])\n",
    "        else:\n",
    "            subset_i = {i[0] for i in subsets[i]}\n",
    "            subset_j = {j[0] for j in subsets[j]}\n",
    "\n",
    "        similarity_matrix[i, j] = compare_func(subset_i, subset_j)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "def plot_feature_subsets_matrix(cm, alg_names, title, cmap=plt.cm.Blues):\n",
    "    title += \"\\n\" # add a little margin for the title\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.colorbar()\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(alg_names))\n",
    "    plt.xticks(tick_marks, alg_names, rotation=45)\n",
    "    plt.yticks(tick_marks, alg_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 + 0.1\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        text = \"%.2f\" % cm[i, j]\n",
    "        plt.text(j, i, text,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 backgroundcolor=\"white\",\n",
    "                 #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                 color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=jaccard)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Jaccard similarity between two feature subsets\")\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=intersection_count)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Intersection between two feature subsets\")\n",
    "\n",
    "similarity_matrix = compute_similary_between_subsets(subsets, compare_func=cosine_similarity)\n",
    "plt.figure(figsize=(6, 8))\n",
    "plot_feature_subsets_matrix(similarity_matrix, alg_names, title=\"Cosine similarity between two features subsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from merge.simple.SimpleUnionSubsetMerger import SimpleUnionSubsetMerger\n",
    "\n",
    "susm = SimpleUnionSubsetMerger(subsets)\n",
    "merged_features = susm.merge()\n",
    "\n",
    "print(\"Unique features (union of all subsets): %d over a total of %d \" % (len(merged_features), (N_FEATURES_ALGORITHM * len(subsets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the merged subset\n",
    "Once we have a merged list containing the best features, we would like to evaluate it with several classifiers\n",
    "\n",
    "_TODO_: use a separate test set ? -> split again train/test set -> no changes in the Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_list_unique = len(merged_features) == len(set(merged_features))\n",
    "print(\"is list unique\", is_list_unique)\n",
    "\n",
    "merged_features = list(merged_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = MLPClassifier(solver=\"adam\", alpha=1e-3, hidden_layer_sizes=(100, 50), activation=\"relu\")\n",
    "#clf = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', early_stopping=False,\n",
    "#       epsilon=1e-08, hidden_layer_sizes=(100,50), learning_rate='constant',\n",
    "#       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
    "#       nesterovs_momentum=True, power_t=0.5, shuffle=True,\n",
    "#       solver='lbfgs', tol=0.0001, verbose=False,\n",
    "#       warm_start=False)\n",
    "\n",
    "scores = cross_val_score(clf, ds.get_X_test()[:, merged_features], ds.get_y_test(), cv=3, n_jobs=-1)\n",
    "score = np.mean(scores)\n",
    "\n",
    "print(\"Score using the merged list of features: %.3f\" % score)\n",
    "\n",
    "assessment_scores[\"MLP\"] = score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(clf, ds.get_X_test()[:, merged_features], ds.get_y_test(), cv=3, n_jobs=-1)\n",
    "score = np.mean(scores)\n",
    "\n",
    "print(\"Score using the merged list of features: %.3f\" % score)\n",
    "\n",
    "assessment_scores[\"ExtraTrees\"] = score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "scores = cross_val_score(clf, ds.get_X_test()[:, merged_features], ds.get_y_test(), cv=3, n_jobs=-1)\n",
    "score = np.mean(scores)\n",
    "\n",
    "print(\"Score using the merged list of features: %.3f\" % score)\n",
    "\n",
    "assessment_scores[\"KNN\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def assess_merged_features(clf, clf_name, assessment_scores):\n",
    "    clf.fit(ds.get_X_train(), ds.get_y_train())\n",
    "    y_pred = clf.predict(ds.get_X_test())\n",
    "    y_test = ds.get_y_test()\n",
    "\n",
    "    scores = cross_val_score(clf, ds.get_X_test()[:, merged_features], ds.get_y_test(), cv=3, n_jobs=-1)\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    print(\"[%s] Score using the merged list of features: %.3f\" % (clf_name, score))\n",
    "\n",
    "    assessment_scores[clf_name] = score, (y_test, y_pred)\n",
    "\n",
    "    \n",
    "assessment_scores  = {}\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "assess_merged_features(clf, \"KNN\", assessment_scores)\n",
    "\n",
    "#clf = MLPClassifier(solver=\"adam\", alpha=1e-3, hidden_layer_sizes=(100, 50), activation=\"relu\")\n",
    "clf = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', early_stopping=False,\n",
    "       epsilon=1e-08, hidden_layer_sizes=(100,50), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, verbose=False,\n",
    "       warm_start=False)\n",
    "\n",
    "assess_merged_features(clf, \"MLP\", assessment_scores)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n",
    "assess_merged_features(clf, \"ExtraTrees\", assessment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import math\n",
    "\n",
    "class_names = range(len(set(ds.get_y())))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "n_subplots = len(assessment_scores)\n",
    "cols = 3\n",
    "rows = int(math.ceil(n_subplots / cols))\n",
    "i = 1\n",
    "\n",
    "for name, score_cm in assessment_scores.iteritems():\n",
    "    y_test, y_pred = score_cm[1]\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.subplot(rows, cols, i)\n",
    "    i += 1\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix for %s' % name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
